{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import bookChapterPageNumbers as chaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get counts and remove stop words\n",
    "def vectorizeText(inputText):\n",
    "    cv = CountVectorizer(stop_words='english')\n",
    "    X = cv.fit_transform(inputText)\n",
    "    df_cv = pd.DataFrame(X.toarray(), columns=cv.get_feature_names())  \n",
    "    df_cv.info()\n",
    "    print(df_cv.columns)\n",
    "    print(df_cv)\n",
    "\n",
    "# get term frequencies and remove stop words\n",
    "def vectorizeTextIDF(inputText):\n",
    "    cv_tfidf = TfidfVectorizer(stop_words='english')\n",
    "    X_tfidf = cv_tfidf.fit_transform(page_sentences)\n",
    "    df_tfidf = pd.DataFrame(X_tfidf.toarray(), columns=cv_tfidf.get_feature_names())\n",
    "    df_tfidf.info()\n",
    "    print(df_tfidf.columns)\n",
    "    print(df_tfidf)      \n",
    "\n",
    "\n",
    "def cleanText(page_text):\n",
    "    \n",
    "    clean_text = page_text\n",
    "    \n",
    "    # remove punctuation\n",
    "    clean_text = re.sub('[%s]' % re.escape(string.punctuation), ' ', clean_text)\n",
    "    # make lower case\n",
    "    clean_text = clean_text.lower()\n",
    "    \n",
    "    # remove numbers\n",
    "    clean_text = re.sub('\\w*\\d\\w*', ' ', clean_text)\n",
    "      \n",
    "    # tokenize the text either in individual words or sentences\n",
    "    clean_text = word_tokenize(clean_text)\n",
    "#     page_sentences = sent_tokenize(clean_text) # this doesn't work if punctuation has already been removed, can't clean the punctuation with the same line of code as above once it's been tokenized\n",
    "\n",
    "    # remove punctuation which has been reintroduced after tokenizing\n",
    "    clean_text = [word for word in clean_text if word.isalpha()]\n",
    "\n",
    "    # remove stop words (can do this manually or automatically with the vectorizer)\n",
    "    clean_text = [word for word in clean_text if word not in stop_words]\n",
    "    \n",
    "    # stem words\n",
    "    clean_text = [porter.stem(word) for word in clean_text]\n",
    "\n",
    "    \n",
    "#     print(clean_text)\n",
    "#     print(page_sentences)\n",
    "    \n",
    "#     print('Num words on page = ', len(page_words))\n",
    "#     print('Num sentences on page = ', len(page_sentences))\n",
    "    \n",
    "#     twograms = list(ngrams(page_words,2))\n",
    "#     print(twograms)\n",
    "\n",
    "    return clean_text\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = [\"../StormlightArchiveBooks/The_Way_of_Kings_The_Stormlight_Archive_Book_1.pdf\"]\n",
    "book_chapters_pages = [chaps.book_1_chapter_pages, chaps.book_2_chapter_pages, chaps.book_3_chapter_pages, chaps.book_4_chapter_pages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chap:  0  starting page: 12  ending page:  17\n",
      "chap:  1  starting page: 18  ending page:  36\n",
      "chap:  2  starting page: 37  ending page:  50\n",
      "chap:  3  starting page: 51  ending page:  66\n",
      "chap:  4  starting page: 67  ending page:  82\n",
      "chap:  5  starting page: 83  ending page:  95\n",
      "chap:  6  starting page: 96  ending page:  109\n",
      "chap:  7  starting page: 110  ending page:  131\n",
      "chap:  8  starting page: 132  ending page:  150\n",
      "chap:  9  starting page: 151  ending page:  172\n",
      "chap:  10  starting page: 173  ending page:  180\n",
      "chap:  11  starting page: 181  ending page:  188\n",
      "chap:  12  starting page: 189  ending page:  200\n",
      "chap:  13  starting page: 201  ending page:  207\n",
      "chap:  14  starting page: 208  ending page:  212\n",
      "chap:  15  starting page: 213  ending page:  220\n",
      "chap:  16  starting page: 221  ending page:  244\n",
      "chap:  17  starting page: 245  ending page:  255\n",
      "chap:  18  starting page: 256  ending page:  267\n",
      "chap:  19  starting page: 268  ending page:  296\n",
      "chap:  20  starting page: 297  ending page:  313\n",
      "chap:  21  starting page: 314  ending page:  344\n",
      "chap:  22  starting page: 345  ending page:  360\n",
      "chap:  23  starting page: 361  ending page:  378\n",
      "chap:  24  starting page: 379  ending page:  381\n",
      "chap:  25  starting page: 382  ending page:  395\n",
      "chap:  26  starting page: 396  ending page:  413\n",
      "chap:  27  starting page: 414  ending page:  431\n",
      "chap:  28  starting page: 432  ending page:  443\n",
      "chap:  29  starting page: 444  ending page:  453\n",
      "chap:  30  starting page: 454  ending page:  473\n",
      "chap:  31  starting page: 474  ending page:  500\n",
      "chap:  32  starting page: 501  ending page:  531\n",
      "chap:  33  starting page: 532  ending page:  540\n",
      "chap:  34  starting page: 541  ending page:  547\n",
      "chap:  35  starting page: 548  ending page:  559\n",
      "chap:  36  starting page: 560  ending page:  580\n",
      "chap:  37  starting page: 581  ending page:  592\n",
      "chap:  38  starting page: 593  ending page:  599\n",
      "chap:  39  starting page: 600  ending page:  615\n",
      "chap:  40  starting page: 616  ending page:  633\n",
      "chap:  41  starting page: 634  ending page:  640\n",
      "chap:  42  starting page: 641  ending page:  646\n",
      "chap:  43  starting page: 647  ending page:  664\n",
      "chap:  44  starting page: 665  ending page:  682\n",
      "chap:  45  starting page: 683  ending page:  688\n",
      "chap:  46  starting page: 689  ending page:  700\n",
      "chap:  47  starting page: 701  ending page:  717\n",
      "chap:  48  starting page: 718  ending page:  724\n",
      "chap:  49  starting page: 725  ending page:  741\n",
      "chap:  50  starting page: 742  ending page:  757\n",
      "chap:  51  starting page: 758  ending page:  772\n",
      "chap:  52  starting page: 773  ending page:  796\n",
      "chap:  53  starting page: 797  ending page:  818\n",
      "chap:  54  starting page: 819  ending page:  835\n",
      "chap:  55  starting page: 836  ending page:  847\n",
      "chap:  56  starting page: 848  ending page:  858\n",
      "chap:  57  starting page: 859  ending page:  863\n",
      "chap:  58  starting page: 864  ending page:  871\n",
      "chap:  59  starting page: 872  ending page:  876\n",
      "chap:  60  starting page: 877  ending page:  881\n",
      "chap:  61  starting page: 882  ending page:  891\n",
      "chap:  62  starting page: 892  ending page:  912\n",
      "chap:  63  starting page: 913  ending page:  921\n",
      "chap:  64  starting page: 922  ending page:  939\n",
      "chap:  65  starting page: 940  ending page:  957\n",
      "chap:  66  starting page: 958  ending page:  973\n",
      "chap:  67  starting page: 974  ending page:  1000\n",
      "chap:  68  starting page: 1001  ending page:  1022\n",
      "chap:  69  starting page: 1023  ending page:  1042\n",
      "chap:  70  starting page: 1043  ending page:  1056\n",
      "chap:  71  starting page: 1057  ending page:  1067\n",
      "chap:  72  starting page: 1068  ending page:  1083\n",
      "chap:  73  starting page: 1084  ending page:  1090\n",
      "chap:  74  starting page: 1091  ending page:  1101\n",
      "chap:  75  starting page: 1102  ending page:  1113\n",
      "chap:  76  starting page: 1114  ending page:  1121\n",
      "chap:  77  starting page: 1122  ending page:  1144\n",
      "chap:  78  starting page: 1145  ending page:  1168\n",
      "chap:  79  starting page: 1169  ending page:  1193\n",
      "chap:  80  starting page: 1194  ending page:  1201\n",
      "chap:  81  starting page: 1202  ending page:  1209\n",
      "chap:  82  starting page: 1210  ending page:  1213\n",
      "chap:  83  starting page: 1214  ending page:  1224\n",
      "chap:  84  starting page: 1225  ending page:  1228\n",
      "chap:  85  starting page: 1229  ending page:  1234\n",
      "chap:  86  starting page: 1235  ending page:  1238\n"
     ]
    }
   ],
   "source": [
    "for i, book in enumerate(books):\n",
    "    with pdfplumber.open(book) as pdf:\n",
    "        this_book_chaps_pages = book_chapters_pages[i]\n",
    "        \n",
    "        for chap in range(len(this_book_chaps_pages)-1):\n",
    "            startingPage = this_book_chaps_pages[chap]-1 # minus one to shift pages down by 1 since the pdf.pages object starts at 0\n",
    "            endingPage = this_book_chaps_pages[chap+1]-1-1 # extra minus one for ending page to not include beginning of next chapter\n",
    "            print('chap: ', chap, ' starting page:', startingPage, ' ending page: ', endingPage)\n",
    "    \n",
    "        \n",
    "        \n",
    "#         this_page = pdf.pages[17]\n",
    "#         page_text = this_page.extract_text()\n",
    "#         print(page_text)\n",
    "\n",
    "#         cleaned_page_text = cleanText(page_text)\n",
    "#         print(cleaned_page_text)\n",
    "\n",
    "#         recombined_text = ' '.join(cleaned_page_text)\n",
    "#         print(recombined_text)\n",
    "\n",
    "    #     vectorizeText(recombined_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
